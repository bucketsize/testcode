--------------------------------------------------
== consider 2 layers
L = [i, j]

== for a fully connected feed forward network
y(1) = x(1).w(1,1) + x(2).w(2,1) ... x(i).w(i,1) + b(1)
y(2) = x(1).w(1,2) + x(2).w(2,2) ... x(i).w(i,2) + b(2)
...
y(j) = x(1).w(1,j) + x(2).w(2,j) ... x(i).w(i,j) + b(j)

=> Y(j,1) = W(j,i) . X(i,1) + B(j,1)
=> Y = 	W.X + B

where

	Y = 	|y(1)|
			|..  |
			|y(j)|

	W =     |w(1,1) .. w(1,i)|
			|..   		     |
			|w(j,1) .. w(j,i)|

	B = 	|b(1)|
			|..  |
			|b(j)|

    X =     |x(i)|    (for layer 1 inputs)
	        |..  |
	        |x(i)|

== for next layer
X' = sigmoid(Y)

== cost
Y(m) = FF(X(m), W, B)    (m = [1..M]; M = batch size
C(m) = Y(m) - A(m)

C = sigma(m=[1..M],  (1/2M) |Y - A|^2)

== gradient descent

grad(C) = |dC/dw1|
          |dC/dw2|
		  |...   |
		  |dC/db1|
		  |dC/db2|
		  |...   |

randomize W, B

delta(W) = - grad(C) . W
delta(B) = - grad(C) . B

delta(W) = - grad(C) . lR    (lR = learning rate
delta(B) = - grad(C) . lR    (lR = learning rate



--------------------------------------------------
consider Z layer Ffnn
L = [n(i)]   ;i=[1, Z]

B = [b(i+1)] ;i=[1, Z-1]
where
	b = |b(1) 		|
        |..			|
		|b(n(i+1))  |

W = [w(i+1, i)]
where
	w = |w(1,1) ..			   w(1,n(i))|
		|..								|
		|w(n(i+1),1) .. w(n(i+1),n(i))|


--------------------------------------
foot notes
----------

matrix multiplication
A(m, n) x B(n, p) = C(m, p)

ex:
	|1 2 3|  	|1|   |10|
	|4 5 6| x   |2| = |32|
                |3|
